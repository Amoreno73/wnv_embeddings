{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d10d6e5",
   "metadata": {},
   "source": [
    "#### SHAP Parameter Importance Ranking  \n",
    "by: Angel Moreno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17021526",
   "metadata": {},
   "source": [
    "Below I am defining the predictor variables and target variable for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f335f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from contextlib import redirect_stdout\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "from catboost import CatBoostRegressor\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import shap\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de97e028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['County', 'GEOID', 'A00', 'A01', 'A02', 'A03', 'A04', 'A05', 'A06',\n",
      "       'A07', 'A08', 'A09', 'A10', 'A11', 'A12', 'A13', 'A14', 'A15', 'A16',\n",
      "       'A17', 'A18', 'A19', 'A20', 'A21', 'A22', 'A23', 'A24', 'A25', 'A26',\n",
      "       'A27', 'A28', 'A29', 'A30', 'A31', 'A32', 'A33', 'A34', 'A35', 'A36',\n",
      "       'A37', 'A38', 'A39', 'A40', 'A41', 'A42', 'A43', 'A44', 'A45', 'A46',\n",
      "       'A47', 'A48', 'A49', 'A50', 'A51', 'A52', 'A53', 'A54', 'A55', 'A56',\n",
      "       'A57', 'A58', 'A59', 'A60', 'A61', 'A62', 'A63', 'Cases_2017',\n",
      "       '2017_population', 'Cases_2017_normalized'],\n",
      "      dtype='object')\n",
      "This shape should be uniform across all dfs: (102, 69)\n"
     ]
    }
   ],
   "source": [
    "# master data here\n",
    "df_2017 = pd.read_csv(\"../finished_csvs/master_2017.csv\")\n",
    "df_2018 = pd.read_csv(\"../finished_csvs/master_2018.csv\")\n",
    "df_2019 = pd.read_csv(\"../finished_csvs/master_2019.csv\")\n",
    "df_2020 = pd.read_csv(\"../finished_csvs/master_2020.csv\")\n",
    "df_2021 = pd.read_csv(\"../finished_csvs/master_2021.csv\")\n",
    "df_2022 = pd.read_csv(\"../finished_csvs/master_2022.csv\")\n",
    "df_2023 = pd.read_csv(\"../finished_csvs/master_2023.csv\")\n",
    "df_2024 = pd.read_csv(\"../finished_csvs/master_2024.csv\")\n",
    "\n",
    "dfs = [df_2017, df_2018, df_2019, df_2020,\n",
    "       df_2021, df_2022, df_2023, df_2024]\n",
    "\n",
    "print(df_2017.columns)\n",
    "print(f\"This shape should be uniform across all dfs: {df_2017.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b6dabf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A00', 'A01', 'A02', 'A03', 'A04', 'A05', 'A06', 'A07', 'A08', 'A09', 'A10', 'A11', 'A12', 'A13', 'A14', 'A15', 'A16', 'A17', 'A18', 'A19', 'A20', 'A21', 'A22', 'A23', 'A24', 'A25', 'A26', 'A27', 'A28', 'A29', 'A30', 'A31', 'A32', 'A33', 'A34', 'A35', 'A36', 'A37', 'A38', 'A39', 'A40', 'A41', 'A42', 'A43', 'A44', 'A45', 'A46', 'A47', 'A48', 'A49', 'A50', 'A51', 'A52', 'A53', 'A54', 'A55', 'A56', 'A57', 'A58', 'A59', 'A60', 'A61', 'A62', 'A63']\n",
      "\n",
      "\n",
      "Cases_2017_normalized\n"
     ]
    }
   ],
   "source": [
    "# these will be the same predictor variables for all 8 csv files (2017-2024)\n",
    "# df_2017.columns[2:66]\n",
    "predictors = ['A00', 'A01', 'A02', 'A03', 'A04', 'A05', 'A06',\n",
    "       'A07', 'A08', 'A09', 'A10', 'A11', 'A12', 'A13', 'A14', 'A15', 'A16',\n",
    "       'A17', 'A18', 'A19', 'A20', 'A21', 'A22', 'A23', 'A24', 'A25', 'A26',\n",
    "       'A27', 'A28', 'A29', 'A30', 'A31', 'A32', 'A33', 'A34', 'A35', 'A36',\n",
    "       'A37', 'A38', 'A39', 'A40', 'A41', 'A42', 'A43', 'A44', 'A45', 'A46',\n",
    "       'A47', 'A48', 'A49', 'A50', 'A51', 'A52', 'A53', 'A54', 'A55', 'A56',\n",
    "       'A57', 'A58', 'A59', 'A60', 'A61', 'A62', 'A63']\n",
    "# likewise, this will be the same target variable ('Cases_20XX_normalized')\n",
    "# always at the end of the dfs\n",
    "example_Y = df_2017.columns[-1]\n",
    "\n",
    "print(predictors)\n",
    "print('\\n')\n",
    "print(example_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa32526",
   "metadata": {},
   "source": [
    "#### CatBoost Model Performance\n",
    "\n",
    "CatBoost is a gradient boosting based model that is excellent when there is a lot of categorical data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "701ec587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dfs_testing = [df_2017]\n",
    "# avg_rmse_scores_2017_2024 = []\n",
    "\n",
    "# # keep 5 for now but maybe increase if it runs somewhat fast\n",
    "# kfold = KFold(n_splits=5, shuffle=True, random_state = 27)\n",
    "\n",
    "# count = 0\n",
    "# year = 2017\n",
    "\n",
    "# for df in dfs:\n",
    "#   count += 1\n",
    "#   print(f\"processing {count}/8\")\n",
    "#   # only need to redefine Y var\n",
    "#   X = df[predictors]\n",
    "#   Y = df[df.columns[-1]]\n",
    "#   # https://catboost.ai/docs/en/concepts/parameter-tuning\n",
    "#   catboost = CatBoostRegressor(\n",
    "#     # default is 1000, but I will use 2000 since this is a smaller dataset\n",
    "#     iterations=2000,\n",
    "#     # depth 6 to 10 recommended\n",
    "#     depth=8,\n",
    "#     loss_function='RMSE',\n",
    "#     verbose=False,\n",
    "#     random_seed = 27\n",
    "#   )\n",
    "\n",
    "#   score = cross_val_score(catboost, X, Y, cv=kfold, scoring=\"neg_root_mean_squared_error\")\n",
    "#   pos_score = -score\n",
    "#   avg_rmse_scores_2017_2024.append({\n",
    "#     \"year\":year,\n",
    "#     \"rmse\":pos_score.mean()\n",
    "#   })\n",
    "\n",
    "#   year += 1\n",
    "# # end of model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13633dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'year': 2017, 'rmse': np.float64(1.252681326251258)},\n",
       " {'year': 2018, 'rmse': np.float64(2.0558699845501422)},\n",
       " {'year': 2019, 'rmse': np.float64(1.295106391978416)},\n",
       " {'year': 2020, 'rmse': np.float64(0.2698741270348198)},\n",
       " {'year': 2021, 'rmse': np.float64(0.3082954710613737)},\n",
       " {'year': 2022, 'rmse': np.float64(0.36875699042322657)},\n",
       " {'year': 2023, 'rmse': np.float64(1.1491637791879044)},\n",
       " {'year': 2024, 'rmse': np.float64(0.33702369708109414)}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_rmse_scores_2017_2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb14dec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output:\n",
    "# [{'year': 2017, 'rmse': np.float64(1.252681326251258)},\n",
    "#  {'year': 2018, 'rmse': np.float64(2.0558699845501422)},\n",
    "#  {'year': 2019, 'rmse': np.float64(1.295106391978416)},\n",
    "#  {'year': 2020, 'rmse': np.float64(0.2698741270348198)},\n",
    "#  {'year': 2021, 'rmse': np.float64(0.3082954710613737)},\n",
    "#  {'year': 2022, 'rmse': np.float64(0.36875699042322657)},\n",
    "#  {'year': 2023, 'rmse': np.float64(1.1491637791879044)},\n",
    "#  {'year': 2024, 'rmse': np.float64(0.33702369708109414)}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4c3f0e",
   "metadata": {},
   "source": [
    "#### TabNet Model Performance\n",
    "\n",
    "TabNet is a deep learning model created for tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d27e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tabnet_cv_rmse(X, y, k=5):\n",
    "#   '''\n",
    "#   Ignore early stoppage and record avg rmse for ONE day only. \n",
    "#   '''\n",
    "#   kfold = KFold(n_splits=k, shuffle=True, random_state=27)\n",
    "#   fold_rmses = []\n",
    "#   warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#   # since tabnet does not support sklearn cross validation\n",
    "#   # we will manually compute rmse for each fold\n",
    "#   for train_idx, test_idx in kfold.split(X):\n",
    "#     X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "#     y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "#     model = TabNetRegressor(\n",
    "#       n_d=8, n_a=8, n_steps=3,\n",
    "#       gamma=1.5, lambda_sparse=1e-4,\n",
    "#       seed=27, verbose=0\n",
    "#     )\n",
    "\n",
    "#     # --- suppress TabNet's early stopping printouts ---\n",
    "#     # this is so I can actually record the best RMSE for each year and get the average\n",
    "#     f = io.StringIO()\n",
    "#     with redirect_stdout(f):\n",
    "#       model.fit(\n",
    "#         X_train.values,\n",
    "#         y_train.reshape(-1, 1),\n",
    "#         eval_set=[(X_test.values, y_test.reshape(-1, 1))],\n",
    "#         eval_metric=['rmse'],\n",
    "#         max_epochs=200,\n",
    "#         patience=20,\n",
    "#         batch_size=64,\n",
    "#         virtual_batch_size=32)\n",
    "\n",
    "#     preds = model.predict(X_test.values).reshape(-1)\n",
    "#     rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "#     fold_rmses.append(rmse)\n",
    "\n",
    "#   # return avg 5 fold rmse for current year\n",
    "#   return np.mean(fold_rmses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824c8bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: 2017\n",
      "processing: 2018\n",
      "processing: 2019\n",
      "processing: 2020\n",
      "processing: 2021\n",
      "processing: 2022\n",
      "processing: 2023\n",
      "processing: 2024\n"
     ]
    }
   ],
   "source": [
    "# results = {}\n",
    "# dfs_testing = [df_2017]\n",
    "\n",
    "# year = 2017\n",
    "# for df in dfs:\n",
    "#   print(f\"processing: {year}\")\n",
    "#   X = df[predictors]\n",
    "#   y = df[df.columns[-1]].values\n",
    "#   avg_rmse = tabnet_cv_rmse(X, y)\n",
    "#   results[df.columns[-1]] = avg_rmse\n",
    "#   year += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c940adcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cases_2017_normalized': np.float64(1.0637535650384204),\n",
       " 'Cases_2018_normalized': np.float64(1.815861333367744),\n",
       " 'Cases_2019_normalized': np.float64(1.1135452653746047),\n",
       " 'Cases_2020_normalized': np.float64(0.21372876809420177),\n",
       " 'Cases_2021_normalized': np.float64(0.29463561869059474),\n",
       " 'Cases_2022_normalized': np.float64(0.34777232804185193),\n",
       " 'Cases_2023_normalized': np.float64(1.2375243289851097),\n",
       " 'Cases_2024_normalized': np.float64(0.3293317663979392)}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c669e1bb",
   "metadata": {},
   "source": [
    "Catboost and TabNet final results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84997e4",
   "metadata": {},
   "source": [
    "CatBoost 5-fold cross validation mean rmse for each year: \n",
    "\n",
    "[{'year': 2017, 'rmse': np.float64(1.252681326251258)},     \n",
    " {'year': 2018, 'rmse': np.float64(2.0558699845501422)},    \n",
    " {'year': 2019, 'rmse': np.float64(1.295106391978416)},   \n",
    " {'year': 2020, 'rmse': np.float64(0.2698741270348198)},    \n",
    " {'year': 2021, 'rmse': np.float64(0.3082954710613737)},    \n",
    " {'year': 2022, 'rmse': np.float64(0.36875699042322657)},   \n",
    " {'year': 2023, 'rmse': np.float64(1.1491637791879044)},    \n",
    " {'year': 2024, 'rmse': np.float64(0.33702369708109414)}]   \n",
    "\n",
    "\n",
    "TabNet 5-fold cross validation mean rmse for each year: \n",
    "\n",
    "{'Cases_2017_normalized': np.float64(1.0637535650384204),   \n",
    " 'Cases_2018_normalized': np.float64(1.815861333367744),    \n",
    " 'Cases_2019_normalized': np.float64(1.1135452653746047),   \n",
    " 'Cases_2020_normalized': np.float64(0.21372876809420177),    \n",
    " 'Cases_2021_normalized': np.float64(0.29463561869059474),    \n",
    " 'Cases_2022_normalized': np.float64(0.34777232804185193),    \n",
    " 'Cases_2023_normalized': np.float64(1.2375243289851097),   \n",
    " 'Cases_2024_normalized': np.float64(0.3293317663979392)}   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1662b1",
   "metadata": {},
   "source": [
    "So in general, the performance is very similar between the two. CatBoost is better (lower RMSE values for a given year) in some years while TabNet is very close in performance most years. Overall, TabNet seems to be producing the lower RMSE overall. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf64add",
   "metadata": {},
   "source": [
    "#### SHAP-TabNet Ranking for 2017-2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef3c974",
   "metadata": {},
   "source": [
    "Now that I have seen and recorded TabNet's performance against CatBoost, I will now produce the SHAP plots for 2017-2024.       \n",
    "The plots will be beeswarm and  bar plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58a436ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training TabNet for 2017\n",
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 14 and best_val_0_rmse = 0.95068\n",
      "Plotting SHAP values for 2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\slplm\\Desktop\\embeddings\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training TabNet for 2018\n",
      "\n",
      "Early stopping occurred at epoch 31 with best_epoch = 11 and best_val_0_rmse = 1.72307\n",
      "Plotting SHAP values for 2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\slplm\\Desktop\\embeddings\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training TabNet for 2019\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 2 and best_val_0_rmse = 0.57773\n",
      "Plotting SHAP values for 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\slplm\\Desktop\\embeddings\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training TabNet for 2020\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_0_rmse = 0.16206\n",
      "Plotting SHAP values for 2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\slplm\\Desktop\\embeddings\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training TabNet for 2021\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_val_0_rmse = 0.63656\n",
      "Plotting SHAP values for 2021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\slplm\\Desktop\\embeddings\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training TabNet for 2022\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_0_rmse = 0.57258\n",
      "Plotting SHAP values for 2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\slplm\\Desktop\\embeddings\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training TabNet for 2023\n",
      "\n",
      "Early stopping occurred at epoch 80 with best_epoch = 60 and best_val_0_rmse = 1.41603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\slplm\\Desktop\\embeddings\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting SHAP values for 2023\n",
      "training TabNet for 2024\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_val_0_rmse = 0.32622\n",
      "Plotting SHAP values for 2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\slplm\\Desktop\\embeddings\\.venv\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "year = 2017\n",
    "# dfs_testing = [df_2017]\n",
    "\n",
    "for df in dfs:\n",
    "    print(f\"training TabNet for {year}\")\n",
    "    ##### same as cv section #####\n",
    "    X = df[predictors]\n",
    "    Y = df[df.columns[-1]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=27)\n",
    "\n",
    "    model = TabNetRegressor(\n",
    "        n_d=8, n_a=8, n_steps=3,\n",
    "        gamma=1.5, lambda_sparse=1e-4,\n",
    "        seed=27, verbose=0\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train.values,\n",
    "        y_train.values.reshape(-1, 1),\n",
    "        eval_set=[(X_test.values, y_test.values.reshape(-1, 1))],\n",
    "        eval_metric=['rmse'],\n",
    "        max_epochs=200,\n",
    "        patience=20,\n",
    "        batch_size=64,\n",
    "        virtual_batch_size=32\n",
    "    )\n",
    "    ##### end training #####\n",
    "\n",
    "    shap_values, masks = model.explain(X_test.values)\n",
    "\n",
    "    mean_abs_shap = pd.Series(np.mean(np.abs(shap_values), axis=0), index=X_test.columns)\n",
    "    shap_normalized = mean_abs_shap / mean_abs_shap.sum()\n",
    "    # ignoring sorting for now\n",
    "    # mean_abs_shap = mean_abs_shap.sort_values(ascending=False)\n",
    "\n",
    "    print(f\"Plotting SHAP values for {year}\")\n",
    "\n",
    "    # save path for the plots\n",
    "    beeswarm_summary_path = \"../shap_plots/beeswarm_plots/\"\n",
    "    bar_plots_path = \"../shap_plots/bar_plots/\"\n",
    "\n",
    "    os.makedirs(beeswarm_summary_path, exist_ok=True)\n",
    "    os.makedirs(bar_plots_path, exist_ok=True)\n",
    "\n",
    "    file_path_beeswarm = os.path.join(beeswarm_summary_path, f'beeswarm_plot_for_{year}.png')\n",
    "    file_path_bar = os.path.join(bar_plots_path, f'bar_plot_for_{year}.png')\n",
    "\n",
    "    # first plot -> beeswarm plot for all 64 features \n",
    "    # note: beeswarm plots do not need normalization\n",
    "    plt.title(f'TabNet SHAP Feature Importance - {df.columns[-1].replace(\"_\", \" \")}')\n",
    "    shap_exp = shap.Explanation(values = shap_values, data = X_test.values, feature_names = X_test.columns)\n",
    "    shap.plots.beeswarm(shap_exp, max_display=64, show=False)\n",
    "    # plt.show()\n",
    "    fig = plt.gcf()\n",
    "    plt.savefig(file_path_beeswarm, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    # second plot -> bar plot for all 64 features \n",
    "    plt.figure(figsize=(10,6))\n",
    "    shap_normalized.plot(kind='bar')\n",
    "    plt.title(f'TabNet SHAP Feature Importance - {df.columns[-1].replace(\"_\", \" \")}')\n",
    "    plt.ylabel('Normalized Mean Abs. SHAP value')\n",
    "    plt.xlabel('Features')\n",
    "    # plt.show()\n",
    "    plt.savefig(file_path_bar, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    year += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a4e3b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
